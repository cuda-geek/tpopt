<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>Pragmatic optimization in modern programming</title>
    <meta name="description" content="Pragmatic optimization in modern programming">
    <meta name="author" content="geek">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
    <link rel="stylesheet" href="css/reveal.css">
    <link rel="stylesheet" href="css/theme/beige.css" id="theme">
    <link rel="stylesheet" href="css/customization.css">
    <link rel="stylesheet" href="lib/css/zenburn.css">
    <link rel="shortcut icon" href="images/ico/favicon.ico">
    <script>
      var link = document.createElement( 'link' );
      link.rel = 'stylesheet';
      link.type = 'text/css';
      link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
      document.getElementsByTagName( 'head' )[0].appendChild( link );
    </script>
    <!--[if lt IE 9]>
    <script src="lib/js/html5shiv.js"></script>
    <![endif]-->
  </head>

  <body>
    <div class="reveal">
      <div class="slides">
        <section>
          <h1>Pragmatic optimization</h1>
          <h2>In modern programming</h2>
          <h3>What every optimizer should know about caches?</h3>
          <p>
            <small>Created by
              <a href="https://github.com/cuda-geek">Marina (geek) Kolpakova</a>
              for
              <a href="unn.ru">UNN</a>
              / 2015-2016
            </small>
          </p>
        </section>

        <section>
          <h2>Course Topics</h2>
          <ul>
            <li><strong>Pragmatics</strong>
              <ul>
                <li>Ordering optimization approaches</li>
                <li>Demystifying a compiler</li>
                <li style="margin-bottom: 20px;">Mastering compiler optimizations</li>
              </ul>
            </li>
            <li><strong>Computer Architectures</strong>
              <ul>
                <li>Modern computer architecture concepts</li>
                <li><b>What every optimizer should know about caches?</b></li>
                <li style="margin-bottom: 20px;">Instruction scheduling</li>
              </ul>
            </li>
<!--             <li><strong>Advanced capabilities</strong>
              <ul>
                <li>SIMD extensions</li>
                <li>Specific co-processors</li>
              </ul>
            </li> -->
          </ul>
        </section>

        <section>
          <h2>Outline</h2>
          <ul>
            <li>The memory-processor gap</li>
            <li>Locality principals</li>
            <li>Cache hierarchy</li>
            <li>Terminology</li>
            <li>Cache line</li>
            <li>Associativity</li>
            <li>Summary of designer chooses</li>
            <li>Cache misses taxonomy</li>
            <li>Replacement policy, Write strategy and Allocation policy</li>
            <li>Write buffer</li>
            <li>summary</li>
          </ul>
        </section>

        <section>
          <h2>The memory-processor gap</h2>
          Processor advances much faster than memory
          <figure>
            <img style="width: 60%" class="simple" src="images/popt/hp-mem.jpg">
            <figcaption><small>Image source Hennessey &amp; Paterson.</small></figcaption>
          </figure>
          <strong>The cache hierarchy goal is mitigating the gap between latencies
          of a memory accesses and processor operations</strong>
        </section>

        <section>
          <h2>Why RAM cannot be fast enough?</h2>
          <dl>
            <dt>Dynamic random-access memory (DRAM)</dt>
            <dd>is used to implement main memory because it is
              <ul>
                <li>cheap to manufacture,</li>
                <li>easy to build large,</li>
              </ul>
              <br>
              but it is slow.</dd>
            <dt>Static random-access memory (SRAM)</dt>
            <dd>cannot be used to implement main memory because it is
              <ul>
                <li>much more expensive to manufacture,</li>
                <li>not capacious (requires more transistors per cell),</li>
              </ul>
              <br>
              but it is much faster.
            </dd>
          </dl>

        </section>

        <section>
          <h2>Locality principals</h2>
          <dl>
            <dt><b>Temporal locality</b></dt>
            <dd>relies on the fact that <strong>recently accessed items</strong> are <strong>likely</strong>
            to be accessed in the <strong>nearest future</strong>. If address <b><code>M(n)</code></b>
            is accessed at time <b><code>t</code></b>, it is likely that <b><code>M(n)</code></b>
            will be used again at time <b><code>t+</code>&epsilon;</b> , where <b>&epsilon;</b>
            is small.</dd>
            <dt><b>Spatial locality</b></dt>
            <dd>relies on the fact that items whose addresses are <strong>near each other likely to accessed close
            together in time</strong>. If address <b><code>M(n)</code></b> is accessed it is likely that
            <b><code>M(n+&epsilon;)</code></b> will be used, where <b><code>&epsilon;</code></b> is small.</dd>
          </dl>
          <!-- <small>Give some examples of localities</small> -->
        </section>

        <section>
          <h2>Cache hierarchy</h2>
          <ul>
            <li><b>Cache</b> is a <strong>small fast</strong> memory usually integrated on the chip that holds recently used data/instructions
            for further use</li>
            <li>Accessing cache is slower than accessing registers, but faster than main memory</li>
            <li>Modern computers have memory of different types and properties organized in hierarchy, several levels
            of caches lay between processor and main memory</li>
            <li> Hierarchy offers a reasonable tread off between <strong>speed</strong>, <strong>size</strong>
             and <strong>cost</strong></li>
            <li>Cache offers shrink in power consumption</li>
            <li>Modern architectures belong to Harvard class so have data and instruction cache separated</li>
          </ul>
        </section>

        <section>
          <section>
            <h2>terminology</h2>
            <dl>
              <dt><b>Access time</b></dt>
              <dd>is the time from when a read or write is requested until it arrives at its destination</dd>

              <dt><b>Cycle time</b></dt>
              <dd>is the minimum time between requests to memory</dd>

              <dt><b>Cache hit</b></dt>
              <dd>is the situation when the requested data is in the cache</dd>

              <dt><b>Cache miss</b></dt>
              <dd style="padding-bottom:0;">is the situation when the requested data isn't in the cache. Upon a cache miss,
              the data is retrieved from main memory (or a higher level of cache) and placed in it</dd>
            </dl>
          </section>

          <section>
            <h2>terminology</h2>
            <dl>
              <dt><b>Cache miss rate</b></dt>
              <dd>is the fraction of cache accesses that result in a miss</dd>

              <dt><b>Local miss rate</b></dt>
              <dd>is the number of misses in a cache divided by the total number of memory access to this cache</dd>

              <dt><b>Global miss rate</b></dt>
              <dd>is the number of misses in a cache divided by the total number of memory accesses generated</dd>
            </dl>

            <small>The <strong>local miss rate</strong> is large for higher level caches because the 1st level
            cache benefits the most from data locality. Thus a <strong>global miss rate</strong> that indicates what
            fraction of the memory access that leave the CPU go all the way to memory is a more useful measure.</small>
          </section>

          <section>
            <h2>terminology</h2>
            <dl>
              <dt><b>Cache miss penalty</b></dt>
              <dd>is the time required for serving a cache miss<br/>(depends on the latency and bandwidth of the memory)</dd>

              <dt><b>Memory stall cycles</b></dt>
              <dd>is the cycles during which a CPU is stalled waiting for a memory access</dd>

              <dt>D$/I$</dt>
              <dd><strong>data</strong> cache/<strong>instruction</strong> cache</dd>
            </dl>
          </section>
        </section>

        <section>
          <h2>Cache line</h2>
          <ul>
            <li>Data transfer unit between registers and the highest cache (L1) is usually in between 1-16 bytes</li>
            <li><b>Cache line</b> is the smallest transfer unit between main memory(or higher level cache) and cache
            (typically 32, 64 bytes, sometimes 128 bytes and more)</li>
            <li>Cache line is also unit of storage allocation in cache</li>
            <li>Modern processors have from 1 to 4 levels of caches
              <ul>
                <li><b>Inclusive</b> caches duplicates the data from higher level cache in lower levels</li>
                <li><b>Exclusive</b> caches contains data only in one level of cache</li>
              </ul>
            </li>
          </ul>
          Most caches are <b>inclusive</b> to simplify coherency handling.
        </section>

        <section>
          <h2>Associativity</h2>
          <blockquote><b>Cache associativity</b> is the number of unique places in a cache where any given memory
          item can reside.</blockquote>
          <ul>
            <li>Whole cache is split into <b>sets</b> equal in size</li>
            <li>Each line in a cache has a <b>tag</b> that indicates the address in memory from which the line has been copied</li>
            <li>A hit is detected through an <b>associative search</b> of all tags</li>
          </ul>
          <blockquote>Consequently, there is a <b>cache size/hit latency tread off</b>, large caches can keep more data,
            but need more time for tag search. <b>L1 have to be small to be fast!</b></blockquote>
        </section>

        <section>
          <h2>Set-Associativity</h2>
          <blockquote>set = address MOD N<small style="vertical-align:bottom;">sets</small></blockquote>
          <img style="width: 60%" class="simple" src="images/popt/set-way.svg">
          <br><small>example of sequential read into cache</small><br>
          Cache size is 512 bytes, line size is 64 bytes, lower 6 bit to store offset in the line, other
          log<small style="vertical-align:bottom">2</small>(N<small style="vertical-align:bottom;">sets</small>)
          is to determine set. The rest bits of the address are used to tag.
          <br>
          T = N<small style="vertical-align:bottom;">address</small> -
          log<small style="vertical-align:bottom">2</small>(Size<small style="vertical-align:bottom;">line</small>) -
          log<small style="vertical-align:bottom">2</small>(N<small style="vertical-align:bottom;">sets</small>)
          <blockquote>cache size = N sets * cache line * Associativity</blockquote>
        </section>

        <section>
          <h2>Associativity: example</h2>
        </section>

        <section>
          <h2>Associativity</h2>
          <ul>
            <li>In an <b>N-way set associative</b> cache, any cache line from memory can map to any of the
            N locations in a <b>set</b>. 4-/8- or 16-way associative caches are often used in modern chips. Used for
            (almost) all moder designs. Cortex-A72 48 KB 3-way L1 I$, 32 KB 2-way L1 I$.</li>
            <li>1 way set associative cache is called <b>direct mapped</b>. Direct mapped means only one location,
            and so high probability of conflict, but tag search is very fast. Example: QC Krait  4 kb L0 I/D$ which
            is used mostly for power saving reasons.</li>
            <li>A fully associative cache is one in which a cache line can be placed anywhere in cache. Full association
            is feasible for very small size caches only (TLB's). can be found in the highest TLB caches.
            Example: Cortex-A72 48-entry instruction, 32-entry data</li>
          </ul>
          <blockquote>Higher associativity gives faster caches, but costly in terms of gates and complexity
          (power and performance).</blockquote>
        </section>

        <section>
          <section>
            <h2>Direct Mapped</h2>
            <img class="simple" width="50%" src="images/popt/dmc.svg"/>
            not much bits on tag are left
          </section>
          <section>
            <h2>Set-associative</h2>
            <img class="simple" width="50%" src="images/popt/sac.svg"/>
          </section>
          <section>
            <h2>Fully-associative</h2>
            <img class="simple" width="50%" src="images/popt/fac.svg"/>
            There are caches which are implemented like this but,

by looking at the numbers for an L2 in use today, will

show that this is impractical. Given a 4MB cache with

64B cache lines the cache would have 65,536 entries.

To achieve adequate performance the cache logic would

have to be able to pick from all these entries the one

matching a given tag in just a few cycles. The effort to
For each cache line a comparator is needed to compare

the large tag (note, S is zero). The letter next to each

connection indicates the width in bits. If none is given

it is a single bit line. Each comparator has to compare

two T-bit-wide values. Then, based on the result, the ap-
propriate cache line content is selected and made avail-
able. This requires merging as many sets of O data lines

as there are cache buckets. The number of transistors

needed to implement a single comparator is large espe-
cially since it must work very fast. No iterative com-
parator is usable. The only way to save on the number

of comparators is to reduce the number of them by iter-
atively comparing the tags. This is not suitable for the

same reason that iterative comparators are not: it takes

too long.
          </section>
        </section>

        <section>
          <h2>Aliases</h2>
        </section>

        <section>
          <h2>managing coherency</h2>
          MESI
          • A dirty cache line is not present in any other pro-
cessor’s cache.

• Clean copies of the same cache line can reside in

arbitrarily many caches.
          <ul>
            <li>Read exclusive</li>
            <li>Read shared</li>
            <li>Write exclusive</li>
            <li>Write shared</li>
            <li>Write dirty</li>
          </ul>
          If a write access is detected and the processor has a clean

copy of the cache line in its cache, this cache line is

marked invalid. Future references will require the cache

line to be reloaded. Note that a read access on another

CPU does not necessitate an invalidation, multiple clean

copies can very well be kept around.

More sophisticated cache implementations allow another

possibility to happen. Assume a cache line is dirty in

one processor’s cache and a second processor wants to

read or write that cache line. In this case the main mem-
ory is out-of-date and the requesting processor must, in-
stead, get the cache line content from the first proces-
sor. Through snooping, the first processor notices this

situation and automatically sends the requesting proces-
sor the data. This action bypasses main memory, though

in some implementations the memory controller is sup-
posed to notice this direct transfer and store the updated

cache line content in main memory. If the access is for

writing the first processor then invalidates its copy of the

local cache line.
        </section>

        <section>
          <h2>Latency benchmark</h2>
        </section>

        <section>
          <h2>Cache misses taxonomy</h2>
          <ol>
            <li><b>Compulsory</b> miss refers to a miss for the <i>very first</i> access for a cache line,
            which is unavoidable for any program.</li>

            <li><b>Capacity</b> miss is the situation than the cache <i>cannot contain all the cache lines</i> needed
            during execution of a program. This type is inevitable for any production program.</li>

            <li><b>Conflict</b> miss occurs when a block is to be discarded and later retrieved because too many cache
            lines mapped to the <i>same set</i>. It is only applicable to a (less than fully) set associative caches.
            Conflict misses may in the worst case lead to a <b>cache trashing</b> where a cache line is repeatedly
            brought in, and then immediately thrown out because of a conflict miss</li>

            <!-- miss forced by hardware coherence protocol -->
            <li><b>Coherency</b> miss occurs when a block must be reloaded from the memory or lower level of cache to keep data
            coherent between multiple processors in <abbr title="Symmetric Multi Processor">SMP</abbr> system. </li>
          </ol>
        </section>

        <section>
          <h2>Summary: Designer's chooses</h2>
          <ul>
            <li>Cache line size
              <ul>
                <li>Decreases compulsory misses</li>
                <li>Usually decreases capacity misses</li>
                <li></li>
                <!-- – Increases conflict misses -->
                <!-- + Dec -->
<!-- ± Increases or decreases capacity misses -->
              </ul>
            </li>
            <li>Cache capacity
              <ul>
                <li>Decreases capacity and some of conflict misses</li>
                <li>Require more time to find data thus increase cost of hit</li>
                <li>It is interesting to note that for the on-die L2 cache a large part (probably even the majority) of the access

time is caused by wire delays. This is a physical lim-
itation which can only get worse with increasing cache

sizes. Only process shrinking (for instance, going from

60nm for Merom to 45nm for Penryn in Intel’s lineup)

can improve those numbers.</li>
              </ul>
            </li>
            <li>Higher associativity
            <ul>
              <li>Decrease number of conflict misses</li>
              <li>Complicates search thus increase cost of hit.</li>
            </ul>
            </li>
          </ul>
        </section>

        <section>
          <h2>Replacement policy</h2>
          <blockquote style="width:100%;"><b>Replacement policy</b> defines which cache line is to be replaced under a miss.</blockquote>
          <ul>
            <li><b>Random</b> evicts cache lines without any preference or specific order</li>
            <li><b>Round robin (RR)</b> evicts cache lines in the FIFO order</li>
            <li><b>Least Recently Used (LRU)</b> evicts cache line which was not in use for the longest time.
            It performs better than random or RR but is more difficult to implement</li>
            <li><b>Pseudo-LRU</b> evicts one of the least recently used. close to LRU in term of cache efficiency,
            but much faster to operate.</li>
            <li><b>Belady</b> evicts block that will be used the furthest in future. Unreachable optimum.</li>
          </ul>
        </section>

        <section>
          <h2>Write strategy</h2>
          <blockquote><b>Write Strategy</b> is a strategy to maintain coherence between caches in a hierarchy
          and the main memory while write occur.</blockquote>
          <p>Data is in the cache (write hit)</p>
          <dl>
            <dt><b>Write through</b></dt>
            <dd>The data is written through all cache hierarchy up to main memory. Memory and cache are always in
            coherent state. Uniform latency on misses, but extra bus traffic.</dd>
            <dt><b>Write back</b></dt>
            <dd>The data is written only to the cache line in the cache. The modified cache line
            is written to memory only when necessary (it need to be replaced by another one). Memory and caches can be
            in not coherent state. Non-uniform miss latency
              <ul>
                <li><b>Clean miss</b> results in one transaction with lower level (fill)</li>
                <li><b>Dirty miss</b> results in two transactions with lower level (writeback & fill)</li>
              </ul>
            </dd>
          </dl>
        </section>

        <section>
          <h2>Allocation policy</h2>
          <blockquote><b>Allocation policy</b> is a policy of a new cache allocation<br> if the write miss occurs.
          </blockquote>
          <p>Data is not in the cache (write miss)</p>

          <dl>
            <dt><b>Write-allocate</b> (is to be used with write back)</dt>
            <dd>Allocate a line in a cache to store by retrieving the data from lower level of cache and update the value.
            It requires <strong>additional bandwidth</strong>, but may <strong>decrease miss rate</strong> for reads.</dd>

            <dt><b>Write-non-allocate</b> (is to be used with write through)</dt>
            <dd>By pass the cache and store the data directly to the memory or lower level of cache. No extra bandwidth
            is added, but might result in miss rate increase.</dd>
          </dl>
        </section>

        <section>
          <h2>Write buffer</h2>
          <blockquote><b>Write buffer</b> is a buffer between the current level of cache and the next level of
          cache or memory</blockquote>
          <ul>
            <li>Store latency is not uniform ( e.g. if a cache line must be read or allocated)</li>
            <li>Buffering writes is used to avoid stalling the CPU. Especially helpful for write-back</li>
            <li>Write buffer (store buffer/write-behind buffer) is a FIFO queue of incomplete stores</li>
            <li>intermediate values can be forwarded for use upon request</li>
          </ul>

<!-- Store A to service load of a value that is still in write buffer avoids unnecessary stalls of load operations

Store B Implies that memory contents are temporarily stale
on a multiprocessor, CPUs see different order of writes
“weak store order”, to be revisited in SMP context -->

        </section>

        <section>
          <section>
            <h2>Hardware cache optimizations</h2>
            <p>Basic:</p>
            <ol>
              <li>Larger block size to reduce miss rate</li>
              <li>Bigger caches to reduce miss rate</li>
              <li>Higher associativity to reduce miss rate</li>
              <li>Multilevel caches to reduce miss penalty</li>
              <li>Giving priority to read misses over writes to reduce miss penalty</li>
              <li>Avoiding address translation during indexing of the cache to reduce hit time</li>
              <li>Victim buffer to reduce conflict misses</li>
            </ol>
          </section>

          <section>
            <h2>Hardware cache optimizations</h2>
            <p>Advanced:</p>
            <ol>
              <li>Small and Simple First-Level Caches to Reduce Hit Time and Power</li>
              <li>Way Prediction to Reduce Hit Time</li>
              <li>Pipelined Cache Access to Increase Cache Bandwidth</li>
              <li><b>Nonblocking Caches to Increase Cache Bandwidth</b></li>
              <li>Multi-banked Caches to Increase Cache Bandwidth</li>
              <li>Critical Word First and Early Restart to Reduce Miss Penalty</li>
              <li>Merging Write Buffer to Reduce Miss Penalty</li>
              <li>Hardware Prefetching of Instructions and Data to Reduce Miss Penalty or Miss Rate</li>
            </ol>
          </section>
        </section>

        <section>
          <h2>Nonblocking Caches</h2>
          <blockquote><b>Nonblocking cache (or Lockup Free)</b> is a cache design for which cache continues to supply
          cache hits during a miss, called <b>hit under miss</b>, or <b>hit under multiple miss</b>.</blockquote>

          <ul>
            <li>Especially useful for out-or-order pipeline</li>
            <li>Out-or-oder with coup of nonblocking cache is generally capable of hiding the  miss penalty of an L1 data cache miss that hits
            in the L2 cache, but is not capable of hiding a significant portion of the L2 miss penalty.</li>
            <li>Hit under miss significantly increases the complexity of the cache controller with the complexity
            increasing as the number of outstanding misses allowed increases.</li>
          </ul>
        </section>

        <section>
          <h2>Victim buffer</h2>
          <blockquote><b>Victim buffer (or Victim cache)</b> is a very small fully-associative cache (&approx;4 entries) used to store
          evicted cache lines before complete kick out. If the cache line in a victim buffer is requested it is placed
          in cache again.</blockquote>
          <ul>
            <li>In fact it is 4 extra ways, shared among all sets</li>
            <li>VB is very useful to deal with conflict misses</li>
            <li>VB helps then associativity is not enough</li>
          </ul>
          for conflict misses
        </section>

        <section>
          <h2>Data prefetching</h2>
          <ul>
            <li>Hardware</li>
            <li>Compiler</li>
            <li>Software</li>
            <li>out-or-order</li>
          </ul>
          for capacity/compulsory misses
        </section>

        <section>
          <h2>Hardware prefetching</h2>
          <blockquote><b>Hardware prefetching</b> is the mechanism implemented in cache HW that puts memory blocks promising to be accessed
           in the nearest future speculatively in cache.</blockquote>
          <ul>
            <li>Hardware is able to recognize different access pattern</li>
            <li>Prefetcher keeps on track multiple read and write streams</li>
            <li>The simplest pattern is streaming kernel.</li>
            <li>The compromise should be found
              <ul>
                <li>Do not evict useful data</li>
                <li>Properly select prefetching distance</li>
              </ul>
            </li>
          </ul>
        </section>

<!--         <section>
          <h2>Coherency problem</h2>
          <p>Cache Contention on SMPs</p>
          <ul>
            <li>When two or more CPUs alternately and repeatedly update the same cache line</li>
            <li>memory contention
              <ul>
                <li>when two or more CPUs update the same variable</li>
                <li>correcting it involves an algorithm change</li>
              </ul>
            </li>
            <li>false sharing
              <ul>
                <li>when CPUs update distinct variables that occupy the same cache line</li>
                <li>correcting it involves modification of data structure layout</li>
              </ul>
            </li>
          </ul>
        </section> -->

        <section>
          <h2>Atomicity</h2>
          <ul>
            <li>Load lock / Store conditional</li>
          </ul>
        </section>

<!--         <section>
          <h2>Memory ordering</h2>
          <ul>
            <li>Compile ordering
              <pre class="cpp"><code>asm volatile("" ::: "memory");</code></pre>
              <pre class="cpp"><code>atomic_signal_fence(memory_order_acq_rel);</code></pre>
              forbids GCC compiler to reorder read and write commands around it
            </li>
            <li>hardware ordering</li>
          </ul>

Sequential consistency (all reads and all writes are in-order)
Relaxed consistency (some types of reordering are allowed)
Loads can be reordered after loads (for better working of cache coherency, better scaling)
Loads can be reordered after stores
Stores can be reordered after stores
Stores can be reordered after loads
Weak consistency (reads and writes are arbitrarily reordered, limited only by explicit memory barriers)
        </section>

        <section>
          <h2>Hardware memory model</h2>
          <blockquote>A <b>hardware memory model</b> defines what kind of memory ordering is to expect at runtime
          relative to an assembly (or machine) code block.</blockquote>
          It is transparent for users, but important when lock-free programming techniques are used.
          <ul>
            <li>Weak - Any load or store operation can effectively be reordered with any other load or store operation,
            as long as it would never modify the behavior of a single, isolated thread</li>
            <li>Strong</li>
            <li>Relaxed</li>
          </ul>
        </section>

        <section>
          <h2>Lock-Free Programming</h2>
        </section> -->

        <section>
          <h2>Pragmatics: caches</h2>
          <ul>
            <li>Don't access for write into the same cache lines from different processes<br>
            &#8594; avoid coherency conflicts</li>
            <li>Ensure data locality, use proper data structures and algorithms<br>
            &#8594; avoid contention conflicts</li>
            <li>Fetch from many different streams carefully<br>
            &#8594; avoid associativity conflicts</li>
            <li>Tile for the largest non-shared cache. Never block for the shared cache size<br>
            &#8594; avoid contention conflicts<br>
            <small>Depending on the speed difference and amount of work per iteration, L1 may be better.</small></li>
            <li>Use software prefetching (<code>PLD</code>)<br>
            &#8594; reduce cold cash misses<br>
            <small>Prefetching suits contiguous memory accesses patterns, but not random</small></li>
            <!-- <li>Compiler Optimizations to Reduce Miss Rate</li> -->
            <!-- <li>Compiler-Controlled Prefetching to Reduce Miss Penalty or Miss Rate</li> -->
          </ul>
        </section>

        <section>
          <h2>Obtaining cache properties ARM</h2>
        </section>

        <section>
          <h2>Optimize for caches</h2>
          <ul>
            <li>Strip mining & tiling </li>
            <li>Stride minimization</li>
          </ul>
        </section>

        <section>
          <h2>Weak and strong memory models</h2>
        </section>
        <section>
          <h2>Permutations for address</h2>
          Skewed caches. mirco-benchmark result
        </section>

        <section>
          <h2>Memory bas</h2>
        </section>

        <section>
          <h2>Optimization for caches</h2>
          <ol>
            <li>Watch your working set</li>
            <li>See the first rule</li>
          </ol>
          <li>Keep inventory of the program small (use less data)</li>
        </section>

        <section>
          <h2>How to confuse prefetcher</h2>
        </section>

        <section>
          <h2>double indirect addressing</h2>
          not good for prefetching since address is not computed yet
        </section>

        <section>
          <h2>images from some benchmark about importance of the knowledge about workset </h2>
        </section>



<!--         <section>
          <h2>Tools</h2>
          pahole
          tag hashing
          streaming and write combining
        </section> -->
        <section>
          <h2>Summary</h2>
            <ul style="width: 100%">
              <li>verity is in the details!</li>
              <li>The second software-hardware gap. Use the patterns which was considered by HW designers</li>
            </ul>
        </section>

        <section>
          <h1>THE END</h1>
          <img alt="The end" class="simple" src="images/popt/infinity.png">
          <h4><a href="https://github.com/cuda-geek">Marina Kolpakova</a> / 2015-2016</h4>
        </section>

      </div>
    </div>
    <script src="lib/js/head.min.js"></script>
    <script src="js/reveal.js"></script>
    <script>
      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        controls: true,
        progress: true,
        history: true,
        center: false,
        slideNumber: true,

        transition: 'slide', // none/fade/slide/convex/concave/zoom

        // Optional reveal.js plugins
        dependencies: [
        { src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
        { src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
        { src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
        { src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
        { src: 'plugin/zoom-js/zoom.js' , async: true  },
        { src: 'plugin/notes/notes.js', async: true }
        ]
      });

    </script>
  </body>
</html>
